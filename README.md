RNN Based Sentence Classification using TensorFlow & Keras

ğŸ“Œ Project Overview

This project demonstrates a simple yet powerful Recurrent Neural Network (RNN) model built using TensorFlow and Keras for binary sentence classification.

The model is trained on a small custom dataset (~30 sentences) and learns to classify text into two categories using deep learning techniques such as tokenization, embeddings, and sequence modeling.

This project helped me understand:

* How RNN works internally
* Text preprocessing pipeline
* Embedding layers
* Binary classification using Deep Learning



Tech Stack

* Python
* TensorFlow
* Keras
* NumPy
* Scikit-learn (if used for train-test split)



ğŸ—ï¸ Model Architecture

The architecture of the model:

1. Text Tokenization

   * Convert sentences into sequences of integers
   * Vocabulary creation
   * Padding to ensure equal input length

2. Embedding Layer

   * Converts integer sequences into dense vector representations
   * Learns word relationships during training

3. Simple RNN Layer

   * Captures sequential dependencies in text
   * Learns contextual meaning from previous words

4. Dense Output Layer

   * 1 neuron
   * **Sigmoid activation**
   * Used for binary classification



ğŸ§® Model Summary (Conceptually)

Input â†’ Tokenization â†’ Padding â†’ Embedding â†’ RNN â†’ Dense (Sigmoid)



âš™ï¸ Training Configuration

* Optimizer: Adam
* Loss Function: Binary Crossentropy
* Evaluation Metric: Accuracy
* Activation Function (Output): Sigmoid



ğŸ“Š Why Sigmoid?

Since this is a binary classification problem, sigmoid outputs a probability between 0 and 1:

* Output close to 0 â†’ Class 0
* Output close to 1 â†’ Class 1



ğŸ“š Key Learning Outcomes

* Understanding how RNN processes sequential data
* Difference between Bag of Words and sequence-based learning
* Importance of Embeddings in NLP
* Role of activation functions in classification
* How loss functions guide model learning



ğŸ“‚ Project Structure


rnn-text-classifier/
â”‚
â”œâ”€â”€ model_training.ipynb
â”œâ”€â”€ rnn_model.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt





ğŸ“ˆ Future Improvements

* Increase dataset size
* Use LSTM or GRU instead of Simple RNN
* Add Dropout for regularization
* Deploy using Streamlit
* Save and load trained model
* Add confusion matrix & precision/recall metrics



## âœ¨ Author

Krenil Barot
AI/ML Enthusiast ğŸš€


